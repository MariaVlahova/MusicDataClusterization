{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MasterThesis.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LehLdP2o-nD"
      },
      "source": [
        "Drive Mounth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m_vjHNVpCC_"
      },
      "source": [
        "#colab\n",
        "# google-drive-ocamlfuseのインストール\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "# Colab用のAuth token作成\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Drive FUSE library用のcredential生成\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "# drive/ を作り、そこにGoogle Driveをマウントする\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXCmaXUkPIvv"
      },
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mnTaUXCpOrW"
      },
      "source": [
        "from google.colab import files\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "drive = \"\"\n",
        "#login function\n",
        "def drive_login():\n",
        "  # Authenticate and create the PyDrive client.\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  return drive\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gjk0E1_pUZm"
      },
      "source": [
        "Install and import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3xE-Nm-pYZG"
      },
      "source": [
        "!pip install git+https://github.com/librosa/librosa\n",
        "!pip install np_utils\n",
        "!pip install keras\n",
        "!pip install pyeasyga\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import glob\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import np_utils\n",
        "import pyeasyga\n",
        "\n",
        "from pyeasyga import pyeasyga\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.cluster import MeanShift\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN9eLpAbrXVf"
      },
      "source": [
        "**Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02c1s7zirYYq"
      },
      "source": [
        "def load_sound_files(file_paths):\n",
        "    raw_sounds = []\n",
        "    for fp in file_paths:\n",
        "        fp=\"drive/app/TUDiplomna/audio/\"+fp\n",
        "        X,sr = librosa.load(fp)\n",
        "        raw_sounds.append(X)\n",
        "    return raw_sounds\n",
        "\n",
        "start = time.time()\n",
        "mypath=\"drive/app/TUDiplomna/audio/\"\n",
        "labelsN=[\"Acoustic_guitar\", \"Applause\", \"Bark\", \"Bass_drum\", \"Burping_or_eructation\", \"Bus\", \"Cello\", \"Chime\", \"Clarinet\", \"Computer_keyboard\"]\n",
        "onlyfiles = []\n",
        "tr_labels=pd.read_csv(\"drive/app/TUDiplomna/train.csv\" , error_bad_lines=False)\n",
        "for f in range(len(tr_labels)):\n",
        "  if tr_labels.iloc[f].label in labelsN:\n",
        "    onlyfiles.append(tr_labels.iloc[f].fname)\n",
        "end = time.time()\n",
        "print(\"Time for data loading:\")\n",
        "print(end - start)\n",
        "print(\"seconds\")\n",
        "random.shuffle(onlyfiles,random.random) #mix files\n",
        "train_files= onlyfiles[:500] \n",
        "test_files= onlyfiles[500:600]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW3GujJ5r2Pp"
      },
      "source": [
        "#Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsTwycG2r8zf"
      },
      "source": [
        "def extract_feature(file_name):\n",
        "    X, sample_rate = librosa.load(file_name)\n",
        "    stft = np.abs(librosa.stft(X))\n",
        "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
        "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
        "    mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
        "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
        "    #tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X),sr=sample_rate).T,axis=0)\n",
        "    return mfccs,chroma,mel,contrast\n",
        "\n",
        "def parse_audio_files(files_names):\n",
        "    features, labels = np.empty((0,187)), np.empty(0)\n",
        "    counter=0\n",
        "    for fn in files_names:\n",
        "      try:\n",
        "        mfccs, chroma, mel, contrast = extract_feature(\"drive/app/TUDiplomna/audio/\"+fn)\n",
        "      except Exception as e:\n",
        "        counter=counter+1\n",
        "        continue\n",
        "      ext_features = np.hstack([mfccs,chroma,mel,contrast])\n",
        "      features = np.vstack([features,ext_features])\n",
        "      labels = np.append(labels, fn)\n",
        "    #print(counter)\n",
        "    return features,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfHbhlCQsiUC"
      },
      "source": [
        "def lablesExtraction(files,labels):\n",
        "  labelsSet=[]\n",
        "  for i in range(0, len(files)):\n",
        "    for j in range(0, 9472):#check where this num come from\n",
        "      if files[i]==labels['fname'].values[j]:\n",
        "        label=labels['label'].values[j]\n",
        "        if label != \"Acoustic_guitar\" and label != \"Applause\" and label != \"Bark\" and label != \"Bass_drum\" and label != \"Bus\":\n",
        "          labelsSet.append('None')\n",
        "        else:\n",
        "          labelsSet.append(label)\n",
        "        break\n",
        "  return labelsSet\n",
        "\n",
        "def oneHotEncodder(labels):\n",
        "  encoder = LabelEncoder()\n",
        "  encoder.fit(labels)\n",
        "  encoded_Y = encoder.transform(labels)\n",
        "  return encoded_Y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQHiJ8u9sw-j"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "tr_features, tr_files= parse_audio_files(train_files)\n",
        "ts_features, ts_files = parse_audio_files(test_files)\n",
        "\n",
        "X_train=pd.DataFrame(tr_features)\n",
        "labels_train=lablesExtraction(tr_files,tr_labels)\n",
        "Y_train=oneHotEncodder(labels_train)\n",
        "#pd.DataFrame(tr_labels)\n",
        "\n",
        "X_test=pd.DataFrame(ts_features)\n",
        "labels_test=lablesExtraction(ts_files,tr_labels)\n",
        "Y_test=oneHotEncodder(labels_test)\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time for extracting test and train features and labels:\")\n",
        "print(end - start)\n",
        "print(\"seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk7QxmO79nq_"
      },
      "source": [
        "def makeArrayCategorial(y_data):\n",
        "  dummy_y = np_utils.to_categorical(y_data)\n",
        "  return dummy_y\n",
        "\n",
        "y_train_cat=makeArrayCategorial(Y_train)\n",
        "y_test_cat=makeArrayCategorial(Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XG_9qB4tB8b"
      },
      "source": [
        "#**Classification**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z0RKmZAWUJz"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml9tqbidIhvi"
      },
      "source": [
        "*One layer perceptron*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDYBJLtMtMuM"
      },
      "source": [
        "start = time.time()\n",
        "# create model -Baseline: 53.72% (15.21%)\n",
        "model = Sequential()\n",
        "model.add(Dense(187, input_dim=187,init='uniform', activation='relu'))\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "#Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train_cat, epochs=150, batch_size=10)\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X_test, y_test_cat, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "end = time.time()\n",
        "print(\"Time for executing  one layered perceptron\")\n",
        "print(end - start)\n",
        "print(\"seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngNgAFw3Inmc"
      },
      "source": [
        "*Multilayer  perceptron*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GemIUlIDtSEx"
      },
      "source": [
        "start = time.time()\n",
        "model = Sequential()\n",
        "model.add(Dense(18, input_dim=187, init='uniform', activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(18, init='uniform', activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(6, init='uniform', activation='softmax'))\n",
        "#Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train_cat, epochs=150, batch_size=10, verbose=0)\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X_test, y_test_cat, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "end = time.time()\n",
        "print(\"Time for executing  two layers perceptron with drop out\")\n",
        "print(end - start)\n",
        "print(\" in seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZSXaP6QIunP"
      },
      "source": [
        "*Logistic regression*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ssYdgi2tznK"
      },
      "source": [
        "start = time.time()\n",
        "# all parameters not specified are set to their defaults\n",
        "logisticRegr = LogisticRegression()\n",
        "logisticRegr.fit(X_train, labels_train)\n",
        "# Use score method to get accuracy of model\n",
        "score = logisticRegr.score(X_test,labels_test)\n",
        "print(\"Accuracy:\")\n",
        "print(score)\n",
        "end = time.time()\n",
        "print(\"Time for executing linear regression model\")\n",
        "print(end - start)\n",
        "print(\" in seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7NSuemX9nAM"
      },
      "source": [
        "*Random Forest*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr6MXLTiEG6P"
      },
      "source": [
        "st= time.time()\n",
        "classifier = RandomForestClassifier(n_estimators=50, max_depth=50, random_state=1)\n",
        "classifier.fit(X_train, Y_train)\n",
        "score=classifier.score(X_test, Y_test)\n",
        "print(\"Accuracy:\")\n",
        "print(score)\n",
        "print ('Time for executing Random Forest: '+str(int(time.time()-st)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm5FoErhI6wZ"
      },
      "source": [
        "*Recurent Neural Network*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIhM5xieJ73B"
      },
      "source": [
        "#LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import AveragePooling1D\n",
        "from keras.layers import Input, Flatten\n",
        "data_dim = 187\n",
        "timesteps = 1\n",
        "num_classes = 6\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(187, return_sequences=True,\n",
        "               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 30\n",
        "model.add(LSTM(90, return_sequences=True))  # returns a sequence of vectors of dimension 30\n",
        "model.add(LSTM(42))  # return a single vector of dimension 30\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "X_train_LSTM = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_LSTM = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "model.fit(X_train_LSTM, y_train_cat, batch_size = 100, epochs = 60, verbose = 0) #verbose =0 -bez history,1 s history\n",
        "start = time.time()\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X_test_LSTM, y_test_cat, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "end = time.time()\n",
        "print(\"Time for executing  two layers perceptron with drop out\")\n",
        "print(end - start)\n",
        "print(\" in seconds\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVEi7_S6t_xh"
      },
      "source": [
        "#Clusterization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y186CpyCXb6"
      },
      "source": [
        "X=X_train\n",
        "print (X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uX6EeGUDYcf"
      },
      "source": [
        "MeanShift"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJs7niVKt91Y"
      },
      "source": [
        "start = time.time()\n",
        "#https://scikit-learn.org/stable/modules/clustering.html#mean-shift\n",
        "clustering = MeanShift()\n",
        "cluster_labels = clustering.fit_predict(X)\n",
        "print(\"Best cluster num is :\", len(list(set(cluster_labels))))\n",
        "silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "print(\"The average silhouette_score is :\", silhouette_avg)\n",
        "end = time.time()\n",
        "print(\"Time for executing mean shift model\")\n",
        "print(end - start)\n",
        "print(\" in seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKYtGq8ouLmx"
      },
      "source": [
        "# define a fitness function -da q napisha po dobre\n",
        "def fitness(individual, data):\n",
        "    temp=0.77;\n",
        "    cNum=1\n",
        "    for x in data:\n",
        "      if temp / float(3.5) > x['score']:\n",
        "        temp=x['score']\n",
        "        cNum=x['clusterNum']\n",
        "    return cNum\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwSNpdJHDa-_"
      },
      "source": [
        "KMeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zrdy2cevRFw"
      },
      "source": [
        "start = time.time()\n",
        "#KMeans(random init)\n",
        "data=[];\n",
        "for i in range(2,20):\n",
        "  cluster_labels = KMeans(n_clusters=i, init='random', random_state=10).fit_predict(X)\n",
        "  data.append({'clusterNum': i , 'score': silhouette_score(X, cluster_labels)})\n",
        "  \n",
        "# initialise the GA with data\n",
        "ga = pyeasyga.GeneticAlgorithm(data)\n",
        "ga.fitness_function = fitness               # set the GA's fitness function\n",
        "ga.run()                                    # run the GA\n",
        "bestSolution=ga.best_individual()           # print the GA's best solution\n",
        "for x in data:\n",
        "    if x['clusterNum'] == bestSolution[0]:\n",
        "        silhouette_avg=x['score']\n",
        "        break\n",
        "print(\"Best cluster num is :\", bestSolution[0])\n",
        "print(\"The average silhouette_score is :\", silhouette_avg)\n",
        "end = time.time()\n",
        "print(\"Time for executing Kmeans with GA\")\n",
        "print(end - start)\n",
        "print(\" in seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViGD9QQpDhCv"
      },
      "source": [
        "KMeans++"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXruNdW8vR3M"
      },
      "source": [
        "start = time.time()\n",
        "#KMeans++\n",
        "data=[];\n",
        "for i in range(2,20):\n",
        "  cluster_labels = KMeans(n_clusters=i, init='random').fit_predict(X)\n",
        "  data.append({'clusterNum': i , 'score': silhouette_score(X, cluster_labels)})\n",
        "  \n",
        "# initialise the GA with data\n",
        "ga = pyeasyga.GeneticAlgorithm(data)\n",
        "ga.fitness_function = fitness               # set the GA's fitness function\n",
        "ga.run()                                    # run the GA\n",
        "bestSolution=ga.best_individual()           # print the GA's best solution\n",
        "for x in data:\n",
        "    if x['clusterNum'] == bestSolution[0]:\n",
        "        silhouette_avg=x['score']\n",
        "        break\n",
        "print(\"Best cluster num is :\", bestSolution[0])\n",
        "print(\"The average silhouette_score is :\", silhouette_avg)\n",
        "end = time.time()\n",
        "print(\"Time for executing Kmeans++ with GA\")\n",
        "print(end - start)\n",
        "print(\" in seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}